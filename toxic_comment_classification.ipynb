{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "from nltk import sent_tokenize, word_tokenize, WordNetLemmatizer, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, GlobalMaxPool1D, Bidirectional\n",
    "from keras.models import Model\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "x_train = train_df['comment_text']\n",
    "y_train = train_df[['toxic', 'severe_toxic', 'obscene', 'threat',\n",
    "                    'insult', 'identity_hate']].values\n",
    "x_test = test_df['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))\n",
    "wnl = WordNetLemmatizer()\n",
    "num_words = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_all(comment):\n",
    "    \"\"\"\n",
    "    getting lemmatized tokens from comments\n",
    "    \"\"\"\n",
    "    \n",
    "    # removing punctuation and numbers form the comments\n",
    "    comment = (unidecode(comment)\n",
    "               .lower()\n",
    "               .translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "               .translate(str.maketrans(\"\",\"\", string.digits)))\n",
    "    comment = \" \".join(comment.split())\n",
    "    \n",
    "    # getting leematized token from the comments\n",
    "    for word, tag in pos_tag(word_tokenize(comment)):\n",
    "        if word not in sw:\n",
    "            if tag.startswith(\"NN\"):\n",
    "                yield wnl.lemmatize(word, pos=\"n\")\n",
    "            elif tag.startswith(\"VB\"):\n",
    "                yield wnl.lemmatize(word, pos=\"v\")\n",
    "            elif tag.startswith(\"JJ\"):\n",
    "                yield wnl.lemmatize(word, pos=\"a\")\n",
    "            elif tag.startswith(\"R\"):\n",
    "                yield wnl.lemmatize(word, pos=\"r\")\n",
    "            else:\n",
    "                yield word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenised_x(x, train_flag=False, tokenizer=None):\n",
    "    \"\"\"\n",
    "    getting tokens prepared further for dnn and embedding\n",
    "    \"\"\"\n",
    "#     c = 0\n",
    "    tokenized_comments = []\n",
    "    for comment in x.values:\n",
    "        tokenized_comments.append(\" \".join(lemmatize_all(comment)))\n",
    "#         print(f\"\\r {c}\", end='')\n",
    "#         c += 1\n",
    "    \n",
    "    if train_flag:\n",
    "        tokenizer = Tokenizer(num_words=num_words)\n",
    "        tokenizer.fit_on_texts(list(tokenized_comments))\n",
    "        tokenizer.word_index = {k:v for k, v in tokenizer.word_index.items()\n",
    "                                if v<=num_words}\n",
    "        with open('tokenizer.pickle', 'wb') as f:\n",
    "            pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    tokenized_x = tokenizer.texts_to_sequences(tokenized_comments)\n",
    "    if train_flag:\n",
    "        return tokenized_x, tokenizer\n",
    "    else:\n",
    "        return tokenized_x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_matrix(word_index, emb_dim=300):\n",
    "    \"\"\"\n",
    "    getting embedding matrix for the given dictionary of words\n",
    "    from the Glove embeddings\n",
    "    \"\"\"\n",
    "    emb_matrix = np.zeros((len(word_index)+1, emb_dim))\n",
    "    with open('../../../word_embedding/glove.6B.'+str(emb_dim)+'d.txt',\n",
    "              encoding='UTF8') as f:\n",
    "        for l in f:\n",
    "            arr = l.split()\n",
    "            if arr[0] in word_index:\n",
    "                emb_matrix[word_index[arr[0]]] = arr[1:]\n",
    "    \n",
    "    return emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_x_train, tokenizer = get_tokenised_x(x_train, train_flag=True)\n",
    "tokenised_x_test, _ = get_tokenised_x(x_test, train_flag=False, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('tokenized_x_train', tokenised_x_train.astype(int), fmt='%i')\n",
    "# np.savetxt('tokenized_x_test', tokenised_x_test.astype(int), fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenised_x_train = np.loadtxt('tokenized_x_train', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenised_x_test = np.loadtxt('tokenized_x_test', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tokenizer_word_index.pickle', 'wb') as f:\n",
    "#     pickle.dump(tokenizer.word_index, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tokenizer_word_index.pickle', 'rb') as f:\n",
    "#     tokenizer_wi = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for getting a good no for length of comments to be fed to the LSTM\n",
    "comment_lengths = [len(comt) for comt in tokenised_x_train]\n",
    "plt.hist(comment_lengths, bins=range(1, 510, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding for getting the all the comments of equal length\n",
    "maxlen = 180\n",
    "tokenised_x_train = pad_sequences(tokenised_x_train, maxlen=maxlen)\n",
    "tokenised_x_test = pad_sequences(tokenised_x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"\n",
    "    Model:\n",
    "    Embedding-->Bi-direct-LSTM-->MaxPool-->Dropout-->Dense-->Dropout-->Dense\n",
    "    In the last dense layer, output layer, we have 6 nodes representing the \n",
    "    6 possible classes and outputting the probabilities of each of the\n",
    "    comment categories\n",
    "    \"\"\"\n",
    "    X_inp = Input((maxlen, ))\n",
    "    emb_matrix = get_emb_matrix(tokenizer.word_index, 300)\n",
    "#     emb_matrix = get_emb_matrix(tokenizer_wi, 300)\n",
    "    X = Embedding(num_words+1, 300, weights=[emb_matrix], trainable=True)(X_inp)\n",
    "    X = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(X)\n",
    "    X = GlobalMaxPool1D()(X)\n",
    "#     X = Dropout(0.2)(X)\n",
    "    X = Dense(80, activation='relu')(X)\n",
    "    X = Dropout(0.05)(X)\n",
    "    X = Dense(6, activation='sigmoid')(X)\n",
    "    return Model(inputs=X_inp, outputs=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gourang\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\gourang\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\gourang\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\gourang\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\gourang\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\gourang\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\gourang\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\gourang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 180, 300)          6000300   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 180, 128)          186880    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 80)                10320     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 486       \n",
      "=================================================================\n",
      "Total params: 6,197,986\n",
      "Trainable params: 6,197,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 2190s 15ms/step - loss: 0.0532 - acc: 0.9809 - val_loss: 0.0499 - val_acc: 0.9813\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 2219s 15ms/step - loss: 0.0407 - acc: 0.9841 - val_loss: 0.0462 - val_acc: 0.9825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x206df5ba208>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tokenised_x_train, y_train, epochs=2, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('./trained_models/glove_variational_lstm_lemma_train_epoch_2_densenodout_valid_split_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(tokenised_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.read_csv('sample_submission.csv')\n",
    "# submission.set_index('id', inplace=True)\n",
    "# submission[submission.columns] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission.to_csv('Submission6.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
